# Model Provider Configuration
# Options: openai, ollama, vertexai
LLM_PROVIDER=
EMBEDDING_PROVIDER=

# OpenAI API Key
OPENAI_API_KEY=

# LLM Model Selection
# OpenAI options: gpt-4.1, gpt-4.1-mini, gpt-5.2 (thinking), o3 (reasoning)
# Ollama options: qwen3:8b, gemma3:7b, gemma3:12b, llama3:8b, mistral:7b
# VertexAI options: gemini-2.5-pro, gemini-1.5-pro, gemini-1.5-flash
LLM_MODEL=

# Fallback Model (used when LLM_MODEL is not recognized)
# Should be a model that's available in your setup
LLM_FALLBACK_MODEL=

# Text2Cypher Specific Model (Optional - defaults to LLM_MODEL/LLM_PROVIDER)
# Use a more capable model for Cypher query generation if needed
# Options are the same as LLM_MODEL and LLM_PROVIDER above
TEXT2CYPHER_MODEL=
TEXT2CYPHER_PROVIDER=

# Text2Cypher Iterative Refinement Settings
# Enable iterative verification and correction of generated Cypher queries
TEXT2CYPHER_ENABLE_REFINEMENT=true
# Maximum refinement iterations before returning best effort (default: 3)
TEXT2CYPHER_MAX_ITERATIONS=3

# Verification methods (comma-separated): syntax, execution, llm
# - syntax: Fast regex-based validation (recommended, instant)
# - execution: EXPLAIN-based database schema validation (requires Neo4j connection)
# - llm: LLM semantic validation (slow but thorough, catches semantic issues)
TEXT2CYPHER_VERIFIERS=syntax,execution

# Correction methods (comma-separated): rule_based, llm
# - rule_based: Fast pattern-based fixes for common issues
# - llm: LLM-powered query rewriting (slow but more capable)
TEXT2CYPHER_CORRECTORS=rule_based,llm

# Agentic Text2Cypher (Deep Agent-powered adaptive graph exploration)
# Uses a thinking model to adaptively explore the graph with planning and iteration
# Options: gpt-5.2, o3, gpt-4.1 (or any LLM_MODEL value)
AGENTIC_TEXT2CYPHER_MODEL=gpt-5.2
AGENTIC_TEXT2CYPHER_PROVIDER=openai
# Maximum iterations the agent can perform (default: 10)
AGENTIC_TEXT2CYPHER_MAX_ITERATIONS=10

# Embedding Model Selection  
# OpenAI options: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim), text-embedding-ada-002 (1536 dim)
# Ollama options: nomic-embed-text (768 dim), nomic-text-embed (768 dim)
# VertexAI options: text-embedding-005 (768 dim)
EMBEDDING_MODEL=

# Embedding Dimensions (Optional - auto-detected if not specified)
# Override this ONLY if you need to manually specify dimensions
# Common values: 768 (VertexAI, Ollama nomic), 1536 (OpenAI small/ada-002), 3072 (OpenAI large)
# Leave empty for automatic detection based on EMBEDDING_MODEL
EMBEDDING_DIMENSION=

# Model Parameters
MODEL_TEMPERATURE=0.0
MODEL_SEED=42
MAX_TOKENS=

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
# Request timeout in seconds (default: 600 = 10 minutes for RAGAS)
OLLAMA_REQUEST_TIMEOUT=600
# Keep alive time for Ollama models (default: 10m)
OLLAMA_KEEP_ALIVE=10m

# RAGAS Evaluation Configuration
# Maximum retries for timeout errors (default: 2)
RAGAS_MAX_RETRIES=2
# Timeout per metric in seconds (default: 120)
RAGAS_METRIC_TIMEOUT=120
# Overall evaluation timeout in seconds (default: 600 = 10 minutes)
RAGAS_OVERALL_TIMEOUT=600
# Force sequential processing to avoid parallel job timeouts
RAGAS_MAX_WORKERS=1
RAGAS_DISABLE_PARALLEL=true
# Evaluation context flag for extended timeouts
EVALUATION_CONTEXT=RAGAS

# Advanced Processing Configuration
# Entity Summarization - Concurrent LLM calls for entity AI summaries
# Benchmark: 30 workers = 9.13x speedup (optimal for 200-500 entities)
# Adjust based on API rate limits: 10 (conservative), 20 (balanced), 30 (aggressive)
ENTITY_SUMMARY_MAX_WORKERS=30
# Community Summarization - Concurrent LLM calls for community summaries (default: 10)
COMMUNITY_SUMMARY_MAX_WORKERS=10

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=
NEO4J_DATABASE=neo4j
CLIENT_NEO4J_DATABASE=neo4j