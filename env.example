# Model Configuration for RAG vs GraphRAG Project
# Copy this file to .env and customize the values for your setup

# Model Providers
# Available options: openai, ollama
LLM_PROVIDER=ollama
EMBEDDING_PROVIDER=ollama

# Model Selection
# For Ollama local models, use format like: qwen3:8b, gemma3:7b, llama3:8b, mistral:7b
# For OpenAI models, use: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
LLM_MODEL=qwen3:8b
EMBEDDING_MODEL=nomic-embed-text

# Model Parameters
MODEL_TEMPERATURE=0.0
MODEL_SEED=42
# MAX_TOKENS=1000  # Optional: set maximum tokens for responses

# Provider-specific Settings
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_API_KEY=your-openai-api-key-here

# Fallback Settings (used when specified model is not available)
LLM_FALLBACK_MODEL=qwen3:8b

# Neo4j Configuration (if using graph-based approaches)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-neo4j-password

# ChromaDB Configuration (if using vector-based approaches)
CHROMA_PERSIST_DIRECTORY=./chroma_db
