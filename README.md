# RAG vs GraphRAG: Comprehensive Evaluation Framework

## ğŸ¯ Project Overview

A comprehensive evaluation framework comparing various RAG approaches using the RAGAS evaluation framework with research-based enhancements.

### **ğŸ” Retrieval Approaches**
1. **ChromaDB RAG** - Traditional vector similarity search (`--chroma`)
2. **GraphRAG** - Multi-hop graph traversal with entity resolution (`--graphrag`)
3. **Advanced GraphRAG** - Community detection and element summarization (`--advanced-graphrag`)
4. **Text2Cypher** - Natural language to Cypher query translation (`--text2cypher`)
5. **Neo4j Vector** - Graph database vector search (`--neo4j-vector`)
6. **Hybrid Cypher** - Combined vector + graph traversal (`--hybrid-cypher`)
7. **DRIFT GraphRAG** - Dynamic reasoning with iterative fact-finding (`--drift-graphrag`)

### **ğŸ§  Ontology & Entity Discovery**
- **Research-based corpus sampling** with TF-IDF clustering and stratified selection
- **Domain-aware entity extraction** (financial, medical, legal, technical, academic)
- **Multi-strategy text sampling** for optimal entity type discovery
- **Quality metrics** and performance analysis

### **ğŸ§ª HotpotQA Benchmark Integration**
- **Multi-hop reasoning questions** from the HotpotQA fullwiki dataset (~7,400 questions)
- **Wikipedia corpus** - Articles downloaded and ingested automatically
- **Research-grade evaluation** - Rigorous testing with ground truth answers
- **Multiple presets** from smoke (50 questions) to full (7,400 questions)

All approaches are evaluated using RAGAS framework with automated visualizations and comprehensive performance metrics.

### **ğŸ“š Research Foundation**
- **GraphRAG Patterns**: [Neo4j GraphRAG Field Guide](https://neo4j.com/blog/developer/graphrag-field-guide-rag-patterns/)
- **Microsoft GraphRAG**: [Community Summary Retrievers](https://graphrag.com/reference/graphrag/global-community-summary-retriever/)
- **DRIFT Algorithm**: [Microsoft DRIFT Research](https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/)
- **HotpotQA**: [Multi-hop Question Answering Dataset](https://hotpotqa.github.io/)
- **Entity Discovery**: 2025 research in ontology discovery and active learning

## ğŸ“ Project Structure

```
RAGvsGraphRAG/
â”œâ”€â”€ ğŸ“‚ data_processors/              # Document processing and graph construction
â”‚   â”œâ”€â”€ process_data.py             # ğŸ¯ Main CLI for data processing
â”‚   â”œâ”€â”€ build_graph/                # Graph processor
â”‚   â”‚   â”œâ”€â”€ main_processor.py       # Main orchestrator class
â”‚   â”‚   â”œâ”€â”€ entity_discovery.py     # Research-based entity discovery
â”‚   â”‚   â”œâ”€â”€ text_processing.py      # PDF extraction, chunking, embeddings
â”‚   â”‚   â”œâ”€â”€ graph_operations.py     # Neo4j operations & entity resolution
â”‚   â”‚   â””â”€â”€ README.md               # Technical deep-dive documentation
â”‚   â”œâ”€â”€ chroma_processor.py         # ChromaDB vector processing
â”‚   â””â”€â”€ graph_processor.py          # Legacy processor (use build_graph instead)
â”œâ”€â”€ ğŸ“‚ retrievers/                   # RAG retrieval implementations
â”‚   â”œâ”€â”€ chroma_retriever.py         # ChromaDB vector similarity search
â”‚   â”œâ”€â”€ graph_rag_retriever.py      # Multi-hop graph traversal
â”‚   â”œâ”€â”€ advanced_graphrag_retriever.py # Community-enhanced GraphRAG
â”‚   â”œâ”€â”€ text2cypher_retriever.py    # Natural language to Cypher
â”‚   â”œâ”€â”€ neo4j_vector_retriever.py   # Neo4j vector search
â”‚   â”œâ”€â”€ hybrid_cypher_retriever.py  # Combined vector + graph
â”‚   â”œâ”€â”€ drift_graphrag_retriever.py # Dynamic reasoning approach
â”‚   â””â”€â”€ README.md                   # Retriever usage guide
â”œâ”€â”€ ğŸ“‚ benchmark/                    # Evaluation framework
â”‚   â”œâ”€â”€ ragas_benchmark.py          # ğŸ¯ Main evaluation CLI
â”‚   â”œâ”€â”€ visualizations.py           # Automated chart generation
â”‚   â”œâ”€â”€ benchmark.csv               # Default benchmark dataset
â”‚   â”œâ”€â”€ hotpotqa/                   # HotpotQA benchmark integration
â”‚   â”‚   â”œâ”€â”€ benchmark_pipeline.py   # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ data_loader.py          # HotpotQA + Wikipedia downloader
â”‚   â”‚   â”œâ”€â”€ wiki_ingester.py        # Neo4j graph ingestion
â”‚   â”‚   â”œâ”€â”€ configs.py              # Preset configurations
â”‚   â”‚   â””â”€â”€ README.md               # HotpotQA documentation
â”‚   â””â”€â”€ README.md                   # Benchmarking guide
â”œâ”€â”€ ğŸ“‚ benchmark_outputs/           # Generated results and visualizations
â”œâ”€â”€ ğŸ“‚ data/                        # Cached datasets (HotpotQA, Wikipedia)
â”œâ”€â”€ ğŸ“‚ tests/                       # Test and validation scripts
â”œâ”€â”€ ğŸ“‚ PDFs/                        # Source documents for processing
â”œâ”€â”€ ğŸ“‚ chroma_db/                   # ChromaDB vector store data
â””â”€â”€ ğŸ“„ requirements.txt             # Python dependencies
```

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Copy and configure .env file from template
cp .env_example .env

# Configure your settings in .env:
# - Provider selection (openai, ollama, vertexai)
# - Embedding model selection
# - Neo4j connection details
# - API keys as needed

# See .env_example for all available options
```

**âš ï¸ IMPORTANT**: Different embedding models produce different vector dimensions.
- VertexAI/Ollama: 768 dimensions
- OpenAI (small/ada-002): 1536 dimensions  
- OpenAI (large): 3072 dimensions

**You must use the same embedding model for data ingestion AND querying!**

See [Embedding Dimensions Guide](docs/EMBEDDING_DIMENSIONS.md) for detailed information.

### 2. Start Neo4j Database
```bash
# Using Docker (recommended)
docker run --name neo4j-rag \
    -p 7474:7474 -p 7687:7687 \
    -e NEO4J_AUTH=neo4j/password \
    neo4j:latest
```

### 3. Process Data (Choose One)

#### **Option A: Process Your PDFs**
```bash
# Place PDFs in PDFs/ folder, then:
python data_processors/process_data.py --pdfs
```

#### **Option B: Use HotpotQA Benchmark (Recommended)**
```bash
# The HotpotQA benchmark automatically downloads and ingests Wikipedia articles
# See "Run Evaluation" section below
```

### 4. Run Evaluation

#### **Quick Benchmark (Default CSV)**
```bash
# Compare all RAG approaches (uses default benchmark.csv)
python benchmark/ragas_benchmark.py --all

# Selective testing
python benchmark/ragas_benchmark.py --chroma --graphrag --hybrid-cypher
```

#### **HotpotQA Benchmark**
```bash
# Build database + test (first run - downloads Wikipedia, ingests, then tests)
python -m benchmark.hotpotqa.benchmark_pipeline smoke --build-database

# Test only (subsequent runs - uses existing graph data)
python -m benchmark.hotpotqa.benchmark_pipeline smoke
python -m benchmark.hotpotqa.benchmark_pipeline smoke --retrievers chroma graphrag hybrid_cypher

# Full evaluation (~7400 questions, ~$100+)
python -m benchmark.hotpotqa.benchmark_pipeline full --build-database
```

### 5. View Results
- **Neo4j Browser**: http://localhost:7474 (explore the knowledge graph)
- **Charts**: `benchmark_outputs/` folder (performance comparisons)
- **Detailed Reports**: CSV and JSON outputs with individual Q&A analysis

## âš¡ Global retriever performance benchmarking (before/after)

The repo includes a small harness to benchmark **Advanced GraphRAG global search** before/after optimizations and generate a markdown summary.

### Run benchmarks

```bash
# Optimized (global-only + single-pass, 1 LLM call)
python -m benchmark.perf_global_search --impl optimized --runs 1 --cold-start --strategy single_pass

# Baseline (legacy full-graph + map-reduce)
python -m benchmark.perf_global_search --impl baseline --runs 1 --cold-start --strategy map_reduce
```

### Generate a markdown comparison report

```bash
python -m benchmark.report_global_perf \
  --before benchmark/results/<baseline_json>.json \
  --after benchmark/results/<optimized_json>.json \
  --out benchmark/results/global_before_after_report.md
```

## ğŸ¯ Key Features

### **ğŸ§  Research-Based Entity Discovery**
- **Multi-strategy corpus sampling** with TF-IDF clustering and stratified selection
- **Domain-aware entity extraction** with hints for financial, medical, legal, technical domains
- **Quality metrics** including diversity scores and compression ratios
- **Interactive CLI approval** for discovered entity types

### **ğŸ§ª HotpotQA Benchmark** 
- **~7,400 multi-hop questions** requiring reasoning over multiple Wikipedia articles
- **Automatic Wikipedia download** with intelligent caching
- **Multiple presets** for quick testing to full evaluation
- **Research-grade evaluation** matching academic benchmarks

### **ğŸ” 7 Retrieval Approaches**
- **ChromaDB RAG** - Fast vector similarity search
- **GraphRAG** - Multi-hop graph traversal with entity resolution
- **Advanced GraphRAG** - Community detection and element summarization  
- **Text2Cypher** - Natural language to database queries
- **Neo4j Vector** - Graph database vector search
- **Hybrid Cypher** - Combined vector + graph approach
- **DRIFT GraphRAG** - Dynamic reasoning with iterative refinement

### **ğŸ“Š Comprehensive Evaluation**
- **RAGAS metrics** - Response Relevancy, Factual Correctness, Semantic Similarity
- **Automated visualizations** - Performance charts and heatmaps
- **Detailed reports** - CSV and JSON outputs
- **Human-readable analysis** - Individual Q&A breakdowns

## ğŸ“š Component Documentation

- **[Data Processors](data_processors/README.md)** - Data processing and ingestion guide
- **[Build Graph](data_processors/build_graph/README.md)** - Technical deep-dive on enhanced graph processing
- **[Retrievers](retrievers/README.md)** - Retrieval approaches and usage patterns
- **[Benchmark](benchmark/README.md)** - Evaluation framework and RAGAS integration
- **[HotpotQA](benchmark/hotpotqa/README.md)** - HotpotQA benchmark documentation
- **[Embedding Dimensions](docs/EMBEDDING_DIMENSIONS.md)** - âš ï¸ **IMPORTANT**: Guide for handling different embedding models and dimensions

## ğŸ› ï¸ Requirements

- **Python 3.8+**
- **Neo4j Database** (Docker recommended)
- **OpenAI API Key** (for embeddings and LLM processing)
- **8GB+ RAM** (for larger datasets)
- **Optional**: scikit-learn (for enhanced entity discovery)

### 2. GraphRAG  
Neo4j graph-enhanced vector search with **dynamic entity discovery**. Automatically discovers entity types from your documents with CLI approval. Includes LLM-based entity resolution to merge duplicates.

### 3. Advanced GraphRAG 
Intelligent routing between global community search and local entity search with element summarization and community detection.

### 4. DRIFT GraphRAG 
Iterative refinement algorithm with dynamic follow-ups and multi-depth exploration using NetworkX action graphs.

### 5. Text2Cypher RAG 
Natural language to Cypher query translation with direct Neo4j graph database querying and schema-aware prompt engineering.

### 6. Neo4j Vector RAG 
Pure Neo4j vector similarity search using native vector operations without graph traversal for fast retrieval - good to compare against vector only databases such as ChromaDB.

## ğŸ“ˆ RAGAS Evaluation Framework

### Metrics Overview

The benchmark evaluates all approaches using three key RAGAS metrics:

#### 1. Response Relevancy (0.0-1.0)
How well the generated answer addresses and is relevant to the question asked.

#### 2. Factual Correctness (0.0-1.0)
How factually accurate the response is compared to ground truth reference answers.

#### 3. Semantic Similarity (0.0-1.0)  
How well the meaning of the response matches the expected answer.

### Overall Performance Calculation

The **Average Score** for each approach is calculated as:
```
Average Score = (Response Relevancy + Factual Correctness + Semantic Similarity) / 3
```

## ğŸ§  Dynamic Entity Discovery

The graph processor now features **intelligent entity discovery** that adapts to your document content:

### How It Works
1. **Corpus Analysis**: Analyzes your entire document collection using hybrid sampling (first/last 500 chars + entity-rich patterns)
2. **LLM Proposal**: GPT-4 proposes relevant entity types based on document content
3. **CLI Approval**: You review and approve/modify the proposed entities
4. **Schema Caching**: Approved entities are cached for reuse across runs
5. **Dynamic Extraction**: Extracts only the approved entity types from each document chunk

### How to Use
Simply run the graph processor and it will automatically discover entities from your documents:
```bash
python data_processors/graph_processor.py
```

### Benefits
- **Adaptive**: Discovers entities relevant to your specific domain (contracts, medical, legal, etc.)
- **Consistent**: Single entity schema applied across all documents in a corpus
- **Efficient**: Caches approved schemas to avoid re-discovery
- **User-Controlled**: You approve all entity types before processing

### Example Discovery Output
```
ğŸ” Analyzing corpus with LLM...
ğŸ“‹ Proposed entities: Contract, Vendor, Deliverable, Timeline, Budget, Compliance
âœ… Approve these entities? (y/n/edit): y
ğŸš€ Processing documents with approved entities...
```

## ğŸ“š Customization Guide

### Adding New Documents
1. **Add PDFs**: Place new PDF files in the `PDFs/` directory
2. **Reprocess**: Run your chosen processing command again:
   ```bash
   python data_processors/graph_processor.py  # Reprocesses all PDFs
   ```
3. **Test**: Validate with `python tests/test_ragas_setup.py`

### Custom Benchmark Questions  
1. **Edit questions**: Modify `benchmark/benchmark.csv`:
   ```csv
   question,ground_truth
   Your custom question?,Expected answer here
   ```
2. **Run benchmark**: `python benchmark/ragas_benchmark.py --all`

## ğŸ”§ Development & Testing

### Health Checks
```bash
# Verify all systems working
python tests/check_chromaDB.py      # ChromaDB status
python tests/check_schema.py        # Neo4j schema and statistics  
python tests/test_ragas_setup.py    # All approaches validation
```

## ğŸš€ Next Steps

### Quick Start Checklist
- [ ] Set up environment variables in `.env`
- [ ] Run `pip install -r requirements.txt`
- [ ] Start Neo4j database
- [ ] Run smoke test: `python -m benchmark.hotpotqa.benchmark_pipeline smoke`
- [ ] View results in `benchmark_outputs/hotpotqa/`

## Cheatsheet

```bash
# Quick benchmark with default CSV
python benchmark/ragas_benchmark.py --hybrid-cypher --chroma --graphrag --limit 5
```

#### **HotpotQA Benchmark**

| Preset | Questions | Default Retrievers | Est. Cost | Est. Time |
|--------|-----------|-------------------|-----------|-----------|
| `mini` | 10 | chroma | ~$1 | 5 min |
| `mini_smoke` | 25 | chroma, graphrag | ~$2 | 8 min |
| `smoke` | 50 | chroma, graphrag | ~$5 | 15 min |
| `dev` | 500 | chroma, graphrag, hybrid_cypher, advanced_graphrag | ~$20 | 60 min |
| `full` | ~7400 | all | ~$100+ | 5 hours |

**Test Only (default - uses existing graph data):**
```bash
# Tests retrievers against existing Neo4j data (no database changes)
python -m benchmark.hotpotqa.benchmark_pipeline mini
python -m benchmark.hotpotqa.benchmark_pipeline smoke
python -m benchmark.hotpotqa.benchmark_pipeline mini --retrievers graphrag neo4j_vector
```

**Build Database (downloads Wikipedia + clears DB + ingests + tests):**
```bash
# âš ï¸ CLEARS Neo4j, ingests Wikipedia articles, then tests retrievers
python -m benchmark.hotpotqa.benchmark_pipeline mini --build-database
python -m benchmark.hotpotqa.benchmark_pipeline smoke --build-database
python -m benchmark.hotpotqa.benchmark_pipeline smoke --build-database --retrievers graphrag neo4j_vector
```

**Recommended workflow:**
```bash
# 1. Build database once with desired preset size
python -m benchmark.hotpotqa.benchmark_pipeline smoke --build-database

# 2. Run multiple tests against the same data (fast iteration)
python -m benchmark.hotpotqa.benchmark_pipeline smoke --retrievers graphrag
python -m benchmark.hotpotqa.benchmark_pipeline smoke --retrievers neo4j_vector hybrid_cypher
python -m benchmark.hotpotqa.benchmark_pipeline smoke --retrievers chroma graphrag neo4j_vector
```

**Utilities:**
```bash
# List all available presets
python -m benchmark.hotpotqa.benchmark_pipeline --list-presets
```
